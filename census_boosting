# ---------------------> Importing the required libraries ------------------------------

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ---------------------> Getting the required data ------------------------------------

data = pd.read_csv('census.csv')

# ---------------------> Length of the dataset ----------------------------------------

len(data[data['income'].astype('str') == '>50K'])

# ---------------------> Number of people having more than 50k salary -----------------

len(data[data['income'].astype('str') == '<=50K'])

# ---------------------> Percentage of people having more than 50k salary -----------------

len(data[data['income'].astype('str') == '<=50K'])

# ---------------------> Preprocessing the dataset ---------------------------------------

income_raw = data['income']
feature_raw = data.iloc[:, :-1]

# ---> Transforming skewed variables using MinMax Scaler

skewed = ['capital-gain', 'capital-loss']
feature_raw[skewed] = feature_raw[skewed].apply(lambda x: np.log(x + 1))
numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']

from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
feature_raw[numerical] = sc.fit_transform(feature_raw[numerical])

non_num = ['workclass', 'education_level', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']
sample = pd.get_dummies(feature_raw[non_num])
feature_raw = pd.concat([feature_raw[numerical], sample], axis = 1)

# ---> Encoding the variables 

income_raw = pd.get_dummies(income_raw, drop_first = True)

# ---------------------> Splitting the data ----------------------------------------------

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(feature_raw, income_raw, shuffle = True, test_size = 0.2, random_state = 0)

# -----> Applying Gradient Boost Classifier -------

from sklearn.ensemble import GradientBoostingClassifier
gclassifier = GradientBoostingClassifier(learning_rate = 0.01, n_estimators = 1000, random_state = 0)
gclassifier.fit(X_train, y_train)

y_pred = gclassifier.predict(X_test)

from sklearn.metrics import confusion_matrix, make_scorer, roc_auc_score
cm = confusion_matrix(y_test, y_pred)
accuracy = roc_auc_score(y_test, y_pred)
scorer = make_scorer(roc_auc_score)
# -----> Searching for Best Parameters -------------------------------------------------

parameters = [{'learning_rate' : [0.01, 0.05, 0.1, 0.5],
               'n_estimators' : [250, 500, 750, 1000],
               'min_samples_split' : [2, 4, 6],
               'max_depth' : [2, 3, 4]}]

from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(estimator = gclassifier, scoring = scorer, param_grid = parameters, verbose = 5)
grid.fit(X_train, y_train)

gclassifier = grid.best_estimator_

y_pred1 = gclassifier.predict(X_test)
cm1 = confusion_matrix(y_test, y_pred1)
accuracy1 = roc_auc_score(y_test, y_pred1)

from sklearn.model_selection import cross_val_score
score = cross_val_score(estimator = gclassifier, X = X_train, y = y_train, cv = 10)
print(score.mean())
